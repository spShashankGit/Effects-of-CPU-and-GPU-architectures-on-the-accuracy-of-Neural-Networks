{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db70bdc6",
   "metadata": {},
   "source": [
    "# CIFAR 10 dataset - Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 140 µs (started: 2021-04-16 16:32:18 +02:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662f52f4",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 315 ms (started: 2021-04-16 16:32:18 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4daea7bc",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 262 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef06e55",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 233 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000,3,32,32)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8567005a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 276 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9991da",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 296 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'CIFAR-10-DS/cifar-10-batches-py/'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    x_train = X_train.astype('float32')\n",
    "    x_test = X_test.astype('float32')\n",
    "\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    #return X_train, y_train, X_test, y_test\n",
    "    #return x_train, y_train, X_val, y_val, x_test, y_test \n",
    "\n",
    "    return x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5bfd60",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 547 ms (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# Invoke the above function to get our data.\n",
    "x_train, y_train, x_test, y_test = get_CIFAR10_data()\n",
    "\n",
    "#print('x_train ', type(x_train), x_train.shape)\n",
    "#print('y_train', y_train.shape)\n",
    "\n",
    "#print('x_test', x_test.shape)\n",
    "#print('y_test', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Create batches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 253 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def createBatches(data,batch_size_required):\n",
    "    batch_size = int(len(data)/batch_size_required)\n",
    "    res=[]\n",
    "    #print('Data ',len(data))\n",
    "    for i in range (batch_size):\n",
    "        batched_data = data[i*batch_size_required:i*batch_size_required+batch_size_required]\n",
    "        res.append(batched_data)\n",
    "        #print('Check here', len(batched_data))\n",
    "    res = np.asarray(res)\n",
    "    #print('Res ',len(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 191 ms (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "x_train = createBatches(x_train,64)\n",
    "y_train = createBatches(y_train,64)\n",
    "\n",
    "x_test = createBatches(x_test,64)\n",
    "y_test = createBatches(y_test,64)"
   ]
  },
  {
   "source": [
    "## Convert npArray to tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 422 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "y_train_tensor = torch.as_tensor(y_train)\n",
    "\n",
    "x_test_tensor = torch.as_tensor(x_test)\n",
    "y_test_tensor = torch.as_tensor(y_test)\n",
    "\n",
    "#print('X tensor ', type(x_test_tensor))"
   ]
  },
  {
   "source": [
    "### Setting up the layers of Convolutional neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 291 µs (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "in_size = 3                 # number of channel in the input image\n",
    " \n",
    "hid1_size = 64              # no of output channel from first CNN layer\n",
    "\n",
    "hid2_size = 128             # no of output channel from second CNN layer\n",
    "\n",
    "hid3_size = 256             # no of output channel from third CNN layer\n",
    "\n",
    "hid4_size = 512             # no of output channel from forth CNN layer\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "out_size = len(classes)     # no of categories in the dataset\n",
    "\n",
    "k_conv_size = 3             # 3x3 convolutional kernel\n",
    "\n",
    "conv_stride = 1             # conv stride 1\n",
    "\n",
    "conv_pad = 1                # conv padding 1\n",
    "\n",
    "maxpool_kernel = 2          # maxpool layer kernel size 2 x 2\n",
    "\n",
    "maxpool_stride = 2          # maxpool layer stride 2\n",
    "\n",
    "max_epoch = 1               # Maximun numbe of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 34.9 ms (started: 2021-04-16 16:32:19 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGG_11(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGG_11,self).__init__()\n",
    "        \n",
    "        self.convLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size, stride=conv_stride, padding=conv_pad),    # conv layer\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            #nn.LocalResponseNorm(64),\n",
    "            nn.ReLU(),                              # Activation layer\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid1_size,hid2_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid2_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride), \n",
    "            \n",
    "            #nn.Flatten(0,-1),\n",
    "            \n",
    "            #nn.Linear(512*bth_size, out_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "     \n",
    "        self.fullyConnLayer = nn.Sequential(\n",
    "            nn.Linear(512, out_size),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "            \n",
    "        \n",
    "    def forward(self,x):\n",
    "            out = self.convLayer(x)\n",
    "\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            \n",
    "            out = self.fullyConnLayer(out)\n",
    "            \n",
    "            return out \n",
    "        \n",
    "vgg_11 = VGG_11()"
   ]
  },
  {
   "source": [
    "## Define a Loss function and optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 581 µs (started: 2021-04-16 16:32:20 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(vgg_11.parameters())"
   ]
  },
  {
   "source": [
    "## Train the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,   200] loss: 2.235\n",
      "[1,   400] loss: 1.690\n",
      "[1,   600] loss: 1.457\n",
      "Finished Training\n",
      "time: 13min 11s (started: 2021-04-16 16:32:20 +02:00)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(x_train_tensor, 0):\n",
    "        #inputs, labels = data\n",
    "        \n",
    "        #images = data\n",
    "        #print(data.shape)\n",
    "        inputs = data\n",
    "        labels = y_train_tensor[i]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg_11(inputs)\n",
    "        \n",
    "        #print(type(outputs))\n",
    "        #print(type(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print statistics        \n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "source": [
    "### Accuracy on whole test-dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 58 %\ntime: 23.7 s (started: 2021-04-16 16:47:59 +02:00)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(x_test_tensor, 0):\n",
    "        #images, labels = data\n",
    "        #print('test ', len(data))\n",
    "        images = data\n",
    "        labels = y_test_tensor[i]\n",
    "\n",
    "        outputs = vgg_11(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "source": [
    "### Which classes perform well and which classes did not?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of plane : 78 %\nAccuracy of   car : 80 %\nAccuracy of  bird : 13 %\nAccuracy of   cat : 53 %\nAccuracy of  deer : 38 %\nAccuracy of   dog : 22 %\nAccuracy of  frog : 73 %\nAccuracy of horse : 57 %\nAccuracy of  ship : 75 %\nAccuracy of truck : 65 %\ntime: 23.9 s (started: 2021-04-16 16:48:27 +02:00)\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(x_test_tensor, 0):\n",
    "        #images, labels = data\n",
    "        images = data\n",
    "        labels = y_test_tensor[i]\n",
    "\n",
    "        outputs = vgg_11(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "source": [
    "# END"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}