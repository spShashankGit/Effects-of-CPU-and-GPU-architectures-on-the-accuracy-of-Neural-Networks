{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd07038e78009725a8dcec152f3684570426fe4eb7739139c0594894b1fd85430c1",
   "display_name": "Python"
  },
  "metadata": {
   "interpreter": {
    "hash": "89b1052d6e47d8648fd3cb4f0f4d6438dffafe00232db206c8add318d261f991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 dataset - Numpy\n",
    "import os\n",
    "import platform\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_11(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGG_11,self).__init__()\n",
    "        \n",
    "        # Setting up the layers of Convolutional neural network\n",
    "        in_size = 3                 # number of channel in the input image\n",
    "    \n",
    "        hid1_size = 64              # no of output channel from first CNN layer\n",
    "\n",
    "        hid2_size = 128             # no of output channel from second CNN layer\n",
    "\n",
    "        hid3_size = 256             # no of output channel from third CNN layer\n",
    "\n",
    "        hid4_size = 512             # no of output channel from forth CNN layer\n",
    "\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "        out_size = len(classes)     # no of categories in the dataset\n",
    "\n",
    "        k_conv_size = 3             # 3x3 convolutional kernel\n",
    "\n",
    "        conv_stride = 1             # conv stride 1\n",
    "\n",
    "        conv_pad = 1                # conv padding 1\n",
    "\n",
    "        maxpool_kernel = 2          # maxpool layer kernel size 2 x 2\n",
    "\n",
    "        maxpool_stride = 2          # maxpool layer stride 2\n",
    "\n",
    "        self.convLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size, stride=conv_stride, padding=conv_pad),    # conv layer\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            #nn.LocalResponseNorm(64),\n",
    "            nn.ReLU(),                              # Activation layer\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid1_size,hid2_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid2_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride), \n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(512, out_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self,x):\n",
    "            out = self.convLayer(x)\n",
    "            \n",
    "            return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_11 = VGG_11()\n",
    "file1 = open(\"outputs.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle a data item\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 batch\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000,3,32,32)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full CIFAR-10 dataset\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=0, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    dirname = os.path.dirname(__file__)\n",
    "    cifar10_dir = 'CIFAR-10-DS/cifar-10-batches-py/'\n",
    "    filename = os.path.join(dirname, cifar10_dir)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(filename)\n",
    "\n",
    "    num_training=X_train.shape[0]\n",
    "    num_test=X_test.shape[0]\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    x_train = X_train.astype('float32')\n",
    "    x_test = X_test.astype('float32')\n",
    "\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    #Save dataset in npy files\n",
    "    save('train_data.npy', x_train)\n",
    "    save('train_label.npy', y_train)\n",
    "\n",
    "    save('test_data.npy', x_test)\n",
    "    save('test_label.npy', y_test)\n",
    "\n",
    "    # Loading from npy files\n",
    "    x_train = load('train_data.npy')\n",
    "    y_train = load('train_label.npy')\n",
    "\n",
    "    x_test = load('test_data.npy')\n",
    "    y_test = load('test_label.npy')\n",
    "\n",
    "    #Shuffle data \n",
    "    data = list(zip(x_train, y_train))\n",
    "    np.random.shuffle(data)\n",
    "    x_train, y_train = zip(*data)\n",
    "\n",
    "    # Save the shuffled data\n",
    "    save('train_data.npy', x_train)\n",
    "    save('train_label.npy', y_train)\n",
    "\n",
    "    #return x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom size batches of the dataset\n",
    "def createBatches(data,batch_size_required,device=\"cpu\"):\n",
    "    batch_size = int(len(data)/batch_size_required)\n",
    "    res=[]\n",
    "\n",
    "    #print('data ', data)\n",
    "    for i in range (batch_size):\n",
    "        batched_data = data[i*batch_size_required:i*batch_size_required+batch_size_required]\n",
    "        res.append(batched_data)\n",
    "\n",
    "    #print('CP 7 ', res)\n",
    "    if (device == \"cpu\"):\n",
    "        res = np.asarray(res)\n",
    "\n",
    "    elif (device == \"cuda\"):\n",
    "        print(type(res))\n",
    "        #res = torch.FloatTensor(res)\n",
    "        return res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useLossFunction():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useOptimizerFunction(name):\n",
    "    learning_rate = 0.01                        # Learning rate\n",
    "    momentum_val = 0.9                          # Momentum value\n",
    "\n",
    "    optimizer=''\n",
    "    if(name == 'Adadelta'):\n",
    "        optimizer = optim.Adadelta(vgg_11.parameters())\n",
    "\n",
    "    elif(name=='SGD'):\n",
    "        optimizer = optim.SGD(vgg_11.parameters(), lr=learning_rate, momentum=momentum_val)\n",
    "    \n",
    "    elif(name=='NAG'):\n",
    "        optimizer = optim.SGD(vgg_11.parameters(), lr=learning_rate, momentum=momentum_val, dampening=0, weight_decay=0, nesterov=True)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-600d04a901ca>, line 17)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-600d04a901ca>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    loss.backward()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def trainNetwork(max_epoch, x_train_tensor,y_train_tensor,optimizer,lossFun):\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(x_train_tensor, 0):\n",
    "\n",
    "            inputs = data\n",
    "            labels = y_train_tensor[i]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = vgg_11(inputs)\n",
    "            \n",
    "            print('labels ', labels)\n",
    "            print('labels shape', labels.size())\n",
    "\n",
    "            \n",
    "            loss = lossFun(outputs, labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print statistics        \n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "                \n",
    "                msg = \"loss is \" + str(running_loss / 100) + \" \\n\"\n",
    "                file1.writelines(msg)\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of individual classes and overall dataset\n",
    "def AccuracyOfIndividualClassesAndDataset(x_test_t,y_test_t,bs):\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    print('Class total before ', class_total)\n",
    "    print('CP 6 ', x_test_t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(x_test_t, 0):\n",
    "            #images, labels = data\n",
    "\n",
    "            images = data\n",
    "            labels = y_test_t[i]\n",
    "\n",
    "            outputs = vgg_11(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Class total after ', class_total)\n",
    "        print('Accuracy of %5s : %.2f %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        msg = \"'Accuracy of \" + str(classes[i]) + \"is \" + str(100 * class_correct[i] / class_total[i]) + \" \\n\"\n",
    "        file1.writelines(msg)\n",
    "    \n",
    "    print('Accuracy of the network on the %d test images: %.2f %%' % (len(y_test_t)*bs,100 * correct / total))\n",
    "    #temp = str((len(y_test_t)*bs,100 * correct / total)) + \" \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n    # SGD Optimizer\\n    optimizer=useOptimizerFunction('SGD')\\n    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\\n    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\\n\\n    # NAG optimzer\\n    optimizer=useOptimizerFunction('NAG')\\n    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\\n    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\\n\\n\\n    print('Time required to run the  file', datetime.now() - begin_time)\\n\\n    test2 = str ( (datetime.now() - begin_time) )\\n    file1.writelines( test2 )\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "#Main function\n",
    "def main():\n",
    "    max_epoch_num = 150               # Maximun numbe of epochs\n",
    "    \n",
    "    #Get train and test dataset\n",
    "    #get_CIFAR10_data()\n",
    "\n",
    "\n",
    "    # Load dataset from npy files\n",
    "    x_train = load('train_data.npy')\n",
    "    y_train = load('train_label.npy')\n",
    "\n",
    "    x_test = load('test_data.npy')\n",
    "    y_test = load('test_label.npy')\n",
    "    \n",
    "    #Divide the dataset into small batches\n",
    "    batch_size = 64\n",
    "    x_train = createBatches(x_train,batch_size)\n",
    "    y_train = createBatches(y_train,batch_size)\n",
    "\n",
    "    x_test = createBatches(x_test,batch_size)\n",
    "    y_test = createBatches(y_test,batch_size)\n",
    "\n",
    "\n",
    "    # Convert npArray to tensor\n",
    "    x_train_tensor = torch.as_tensor(x_train)\n",
    "    y_train_tensor = torch.as_tensor(y_train)\n",
    "\n",
    "    x_test_tensor = torch.as_tensor(x_test)\n",
    "    y_test_tensor = torch.as_tensor(y_test)\n",
    "\n",
    "    criterion = useLossFunction()\n",
    "\n",
    "    # Adadelta Optimizer\n",
    "    optimizer=useOptimizerFunction('Adadelta')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\"\"\"\n",
    "    # SGD Optimizer\n",
    "    optimizer=useOptimizerFunction('SGD')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "    # NAG optimzer\n",
    "    optimizer=useOptimizerFunction('NAG')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "\n",
    "    print('Time required to run the  file', datetime.now() - begin_time)\n",
    "\n",
    "    test2 = str ( (datetime.now() - begin_time) )\n",
    "    file1.writelines( test2 )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "labels  tensor([7, 2, 4, 6, 4, 6, 0, 3, 6, 2, 0, 8, 0, 3, 5, 8, 5, 9, 0, 9, 0, 2, 9, 4,\n        5, 5, 1, 9, 9, 7, 2, 4, 2, 6, 6, 1, 2, 6, 4, 1, 0, 9, 5, 9, 5, 0, 0, 5,\n        7, 8, 4, 4, 2, 9, 6, 5, 6, 3, 5, 7, 1, 1, 0, 6])\nlabels shape torch.Size([64])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-c10c089bccad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Adadelta Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0museOptimizerFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adadelta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mAccuracyOfIndividualClassesAndDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \"\"\"\n",
      "\u001b[0;32m<ipython-input-11-89ac524e0554>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(max_epoch, x_train_tensor, y_train_tensor, optimizer, lossFun)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-48715e55d3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# execute only if run as a script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-c10c089bccad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Adadelta Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0museOptimizerFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adadelta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mAccuracyOfIndividualClassesAndDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \"\"\"\n",
      "\u001b[0;32m<ipython-input-22-89ac524e0554>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(max_epoch, x_train_tensor, y_train_tensor, optimizer, lossFun)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-13269c9d7601>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "## Test code here\n",
    "if (torch.cuda.is_available()):\n",
    "        print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        cuda = torch.device('cuda')                                                     # Setting default CUDA device\n",
    "        #score_pytorch_gpu = torch.empty((max_iteration,1),dtype=torch.float32)         # Tensor store on GPU, to conatain Xn and Xn+1\n",
    "        #score_pytorch_gpu = logisticMapPyTorch(x_init, score_pytorch_gpu)              # Calling Logistic Map function with initial values\n",
    "    \n",
    "        x_train = load('./train_data.npy')\n",
    "        x_train_gpu = torch.FloatTensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CP 3 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('CP 3',x_train_gpu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gpu = torch.FloatTensor(x_train).cuda()\n",
    "\n",
    "y_train_gpu = torch.FloatTensor(load('train_label.npy')).cuda()\n",
    "\n",
    "x_test_gpu = torch.FloatTensor(load('test_data.npy')).cuda()\n",
    "\n",
    "y_test_gpu = torch.FloatTensor(load('test_label.npy')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_11 = vgg_11.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n<class 'list'>\n<class 'list'>\n<class 'list'>\nTime required to run the  file 0:00:00.008020\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "begin_time = datetime.now()  \n",
    "\n",
    "#Divide the dataset into small batches'\n",
    "batch_size = 64\n",
    "x_train_gpu = createBatches(x_train_gpu,batch_size, \"cuda\")\n",
    "y_train_gpu = createBatches(y_train_gpu,batch_size, \"cuda\")\n",
    "\n",
    "x_test_gpu = createBatches(x_test_gpu,batch_size, \"cuda\")\n",
    "y_test_gpu = createBatches(y_test_gpu,batch_size, \"cuda\")\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "type(x_train_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time required to run the  file 0:00:00.000657\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "max_epoch_num = 150               # Maximun numbe of epochs\n",
    "# Convert npArray to tensor\n",
    "x_train_tensor =  (x_train_gpu)\n",
    "#x_train_tensor =  torch.as_tensor(x_train_gpu)\n",
    "# torch.as_tensor(x_train)\n",
    "y_train_tensor = y_train_gpu\n",
    "\n",
    "x_test_tensor = x_test_gpu\n",
    "y_test_tensor = y_test_gpu\n",
    "\n",
    "criterion = useLossFunction()\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "labels  tensor([7., 2., 4., 6., 4., 6., 0., 3., 6., 2., 0., 8., 0., 3., 5., 8., 5., 9.,\n        0., 9., 0., 2., 9., 4., 5., 5., 1., 9., 9., 7., 2., 4., 2., 6., 6., 1.,\n        2., 6., 4., 1., 0., 9., 5., 9., 5., 0., 0., 5., 7., 8., 4., 4., 2., 9.,\n        6., 5., 6., 3., 5., 7., 1., 1., 0., 6.], device='cuda:0')\nlabels shape torch.Size([64])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3afd8931fbd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_train_tensor_long\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_tensor_long\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time required to run the  file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbegin_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-89ac524e0554>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(max_epoch, x_train_tensor, y_train_tensor, optimizer, lossFun)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# Adadelta Optimizer\n",
    "optimizer=useOptimizerFunction('Adadelta')\n",
    "\n",
    "x_train_tensor_long = x_train_tensor\n",
    "trainNetwork(max_epoch_num, x_train_tensor_long,y_train_tensor,optimizer,criterion)\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer=useOptimizerFunction('SGD')\n",
    "trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# NAG optimzer\n",
    "optimizer=useOptimizerFunction('NAG')\n",
    "trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)\n",
    "\n",
    "test2 = str ( (datetime.now() - begin_time) )"
   ]
  }
 ]
}