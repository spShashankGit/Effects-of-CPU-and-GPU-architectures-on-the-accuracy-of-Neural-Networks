{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f58f7ec9cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIFAR 10 dataset - Numpy\n",
    "import os\n",
    "import platform\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)                                 # to set same random number to all devices [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_11(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGG_11,self).__init__()\n",
    "        \n",
    "        # Setting up the layers of Convolutional neural network\n",
    "        in_size = 3                 # number of channel in the input image\n",
    "    \n",
    "        hid1_size = 64              # no of output channel from first CNN layer\n",
    "\n",
    "        hid2_size = 128             # no of output channel from second CNN layer\n",
    "\n",
    "        hid3_size = 256             # no of output channel from third CNN layer\n",
    "\n",
    "        hid4_size = 512             # no of output channel from forth CNN layer\n",
    "\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "        out_size = len(classes)     # no of categories in the dataset\n",
    "\n",
    "        k_conv_size = 3             # 3x3 convolutional kernel\n",
    "\n",
    "        conv_stride = 1             # conv stride 1\n",
    "\n",
    "        conv_pad = 1                # conv padding 1\n",
    "\n",
    "        maxpool_kernel = 2          # maxpool layer kernel size 2 x 2\n",
    "\n",
    "        maxpool_stride = 2          # maxpool layer stride 2\n",
    "\n",
    "        self.convLayer = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size, stride=conv_stride, padding=conv_pad),    # conv layer\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            #nn.LocalResponseNorm(64),\n",
    "            nn.ReLU(),                              # Activation layer\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid1_size,hid2_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid2_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid3_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid3_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(hid4_size,hid4_size,k_conv_size, stride=conv_stride, padding=conv_pad),\n",
    "            nn.BatchNorm2d(hid4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=maxpool_kernel,stride=maxpool_stride), \n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(512, out_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self,x):\n",
    "            out = self.convLayer(x)\n",
    "            \n",
    "            return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_11 = VGG_11()\n",
    "file1 = open(\"outputs.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle a data item\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 batch\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000,3,32,32)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full CIFAR-10 dataset\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=0, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    dirname = os.path.dirname(__file__)\n",
    "    cifar10_dir = 'CIFAR-10-DS/cifar-10-batches-py/'\n",
    "    filename = os.path.join(dirname, cifar10_dir)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(filename)\n",
    "\n",
    "    num_training=X_train.shape[0]\n",
    "    num_test=X_test.shape[0]\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    x_train = X_train.astype('float32')\n",
    "    x_test = X_test.astype('float32')\n",
    "\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    #Save dataset in npy files\n",
    "    save('train_data.npy', x_train)\n",
    "    save('train_label.npy', y_train)\n",
    "\n",
    "    save('test_data.npy', x_test)\n",
    "    save('test_label.npy', y_test)\n",
    "\n",
    "    # Loading from npy files\n",
    "    x_train = load('train_data.npy')\n",
    "    y_train = load('train_label.npy')\n",
    "\n",
    "    x_test = load('test_data.npy')\n",
    "    y_test = load('test_label.npy')\n",
    "\n",
    "    #Shuffle data \n",
    "    data = list(zip(x_train, y_train))\n",
    "    np.random.shuffle(data)\n",
    "    x_train, y_train = zip(*data)\n",
    "\n",
    "    # Save the shuffled data\n",
    "    save('train_data.npy', x_train)\n",
    "    save('train_label.npy', y_train)\n",
    "\n",
    "    #return x_train, y_train, x_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom size batches of the dataset\n",
    "def createBatches(data,batch_size_required,device=\"cpu\"):\n",
    "    batch_size = int(len(data)/batch_size_required)\n",
    "    res=[]\n",
    "    \n",
    "    for i in range (batch_size):\n",
    "        batched_data = data[i*batch_size_required:i*batch_size_required+batch_size_required]\n",
    "        res.append(batched_data)\n",
    "\n",
    "\n",
    "    if (device == \"cpu\"):\n",
    "        res = np.asarray(res)\n",
    "\n",
    "    elif (device == \"cuda\"):\n",
    "        return res\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useLossFunction():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useOptimizerFunction(name):\n",
    "    learning_rate = 0.01                        # Learning rate\n",
    "    momentum_val = 0.9                          # Momentum value\n",
    "\n",
    "    optimizer=''\n",
    "    if(name == 'Adadelta'):\n",
    "        optimizer = optim.Adadelta(vgg_11.parameters())\n",
    "\n",
    "    elif(name=='SGD'):\n",
    "        optimizer = optim.SGD(vgg_11.parameters(), lr=learning_rate, momentum=momentum_val)\n",
    "    \n",
    "    elif(name=='NAG'):\n",
    "        optimizer = optim.SGD(vgg_11.parameters(), lr=learning_rate, momentum=momentum_val, dampening=0, weight_decay=0, nesterov=True)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(max_epoch, x_train_tensor,y_train_tensor,optimizer,lossFun):\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(x_train_tensor, 0):\n",
    "\n",
    "            inputs = data\n",
    "            labels = y_train_tensor[i]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = vgg_11(inputs)\n",
    "            \n",
    "            #print('labels ', labels)\n",
    "            #print('labels shape', labels.size())\n",
    "            \n",
    "            labels = labels.to(device=device, dtype=torch.int64)\n",
    "            \n",
    "            #print('After')\n",
    "            #print('labels ', labels)\n",
    "            #print('labels shape', labels.size())\n",
    "\n",
    "            \n",
    "            loss = lossFun(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # print statistics        \n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "                \n",
    "                msg = \"loss is \" + str(running_loss / 100) + \" \\n\"\n",
    "                file1.writelines(msg)\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of individual classes and overall dataset\n",
    "def AccuracyOfIndividualClassesAndDataset(x_test_t,y_test_t,bs):\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    #print('Class total before ', class_total)\n",
    "    #print('CP 6 ', x_test_t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(x_test_t, 0):\n",
    "            #images, labels = data\n",
    "\n",
    "            images = data\n",
    "            labels = y_test_t[i]\n",
    "\n",
    "            outputs = vgg_11(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                \n",
    "                #print('label item()',label.item())\n",
    "                #print(type(c[i].item))\n",
    "                #print('test ', class_correct)\n",
    "                \n",
    "                class_correct[int(label.item())] += c[i].item()\n",
    "                class_total[int(label.item())] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        #print('Class total after ', class_total)\n",
    "        print('Accuracy of %5s : %.2f %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        msg = \"'Accuracy of \" + str(classes[i]) + \"is \" + str(100 * class_correct[i] / class_total[i]) + \" \\n\"\n",
    "        file1.writelines(msg)\n",
    "    \n",
    "    print('Accuracy of the network on the %d test images: %.2f %%' % (len(y_test_t)*bs,100 * correct / total))\n",
    "    #temp = str((len(y_test_t)*bs,100 * correct / total)) + \" \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n    # SGD Optimizer\\n    optimizer=useOptimizerFunction('SGD')\\n    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\\n    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\\n\\n    # NAG optimzer\\n    optimizer=useOptimizerFunction('NAG')\\n    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\\n    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\\n\\n\\n    print('Time required to run the  file', datetime.now() - begin_time)\\n\\n    test2 = str ( (datetime.now() - begin_time) )\\n    file1.writelines( test2 )\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Main function\n",
    "def main():\n",
    "    max_epoch_num = 1               # Maximun numbe of epochs\n",
    "    \n",
    "    #Get train and test dataset\n",
    "    #get_CIFAR10_data()\n",
    "\n",
    "\n",
    "    # Load dataset from npy files\n",
    "    x_train = load('train_data.npy')\n",
    "    y_train = load('train_label.npy')\n",
    "\n",
    "    x_test = load('test_data.npy')\n",
    "    y_test = load('test_label.npy')\n",
    "    \n",
    "    #Divide the dataset into small batches\n",
    "    batch_size = 64\n",
    "    x_train = createBatches(x_train,batch_size)\n",
    "    y_train = createBatches(y_train,batch_size)\n",
    "\n",
    "    x_test = createBatches(x_test,batch_size)\n",
    "    y_test = createBatches(y_test,batch_size)\n",
    "\n",
    "\n",
    "    # Convert npArray to tensor\n",
    "    x_train_tensor = torch.as_tensor(x_train)\n",
    "    y_train_tensor = torch.as_tensor(y_train)\n",
    "\n",
    "    x_test_tensor = torch.as_tensor(x_test)\n",
    "    y_test_tensor = torch.as_tensor(y_test)\n",
    "\n",
    "    criterion = useLossFunction()\n",
    "\n",
    "    # Adadelta Optimizer\n",
    "    optimizer=useOptimizerFunction('Adadelta')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\"\"\" \n",
    "    # SGD Optimizer\n",
    "    optimizer=useOptimizerFunction('SGD')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "    # NAG optimzer\n",
    "    optimizer=useOptimizerFunction('NAG')\n",
    "    trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "    AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "\n",
    "    print('Time required to run the  file', datetime.now() - begin_time)\n",
    "\n",
    "    test2 = str ( (datetime.now() - begin_time) )\n",
    "    file1.writelines( test2 )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-367af49d7a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-203a9080888d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load dataset from npy files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_label.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.npy'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "## Test code here\n",
    "if (torch.cuda.is_available()):\n",
    "        print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        device = torch.device('cuda')                                                     # Setting default CUDA device\n",
    "        #score_pytorch_gpu = torch.empty((max_iteration,1),dtype=torch.float32)         # Tensor store on GPU, to conatain Xn and Xn+1\n",
    "        #score_pytorch_gpu = logisticMapPyTorch(x_init, score_pytorch_gpu)              # Calling Logistic Map function with initial values\n",
    "    \n",
    "        x_train = load('./train_data.npy')\n",
    "        x_train_gpu = torch.FloatTensor(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP 3 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('CP 3',x_train_gpu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gpu = torch.FloatTensor(x_train).cuda()\n",
    "\n",
    "y_train_gpu = torch.FloatTensor(load('train_label.npy')).cuda()\n",
    "\n",
    "x_test_gpu = torch.FloatTensor(load('test_data.npy')).cuda()\n",
    "\n",
    "y_test_gpu = torch.FloatTensor(load('test_label.npy')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_11 = vgg_11.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Time required to run the  file 0:00:00.004839\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "begin_time = datetime.now()  \n",
    "\n",
    "#Divide the dataset into small batches'\n",
    "batch_size = 64\n",
    "x_train_gpu = createBatches(x_train_gpu,batch_size, \"cuda\")\n",
    "y_train_gpu = createBatches(y_train_gpu,batch_size, \"cuda\")\n",
    "\n",
    "x_test_gpu = createBatches(x_test_gpu,batch_size, \"cuda\")\n",
    "y_test_gpu = createBatches(y_test_gpu,batch_size, \"cuda\")\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to run the  file 0:00:00.001178\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "max_epoch_num = 1              # Maximun numbe of epochs\n",
    "# Convert npArray to tensor\n",
    "x_train_tensor =  (x_train_gpu)\n",
    "#x_train_tensor =  torch.as_tensor(x_train_gpu)\n",
    "# torch.as_tensor(x_train)\n",
    "y_train_tensor = y_train_gpu\n",
    "\n",
    "x_test_tensor = x_test_gpu\n",
    "y_test_tensor = y_test_gpu\n",
    "\n",
    "criterion = useLossFunction()\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.165\n",
      "[1,   200] loss: 1.124\n",
      "[1,   300] loss: 1.030\n",
      "[1,   400] loss: 1.019\n",
      "[1,   500] loss: 0.960\n",
      "[1,   600] loss: 0.937\n",
      "[1,   700] loss: 0.902\n",
      "Finished Training\n",
      "Time required to run the  file 0:00:25.893000\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# Adadelta Optimizer\n",
    "optimizer=useOptimizerFunction('Adadelta')\n",
    "\n",
    "x_train_tensor_long = x_train_tensor\n",
    "trainNetwork(max_epoch_num, x_train_tensor_long,y_train_tensor,optimizer,criterion)\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 85.45 %\n",
      "Accuracy of   car : 88.00 %\n",
      "Accuracy of  bird : 64.56 %\n",
      "Accuracy of   cat : 39.73 %\n",
      "Accuracy of  deer : 56.36 %\n",
      "Accuracy of   dog : 55.17 %\n",
      "Accuracy of  frog : 76.79 %\n",
      "Accuracy of horse : 73.02 %\n",
      "Accuracy of  ship : 84.21 %\n",
      "Accuracy of truck : 62.82 %\n",
      "Accuracy of the network on the 9984 test images: 69.42 %\n"
     ]
    }
   ],
   "source": [
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.763\n",
      "[1,   200] loss: 0.766\n",
      "[1,   300] loss: 0.709\n",
      "[1,   400] loss: 0.686\n",
      "[1,   500] loss: 0.662\n",
      "[1,   600] loss: 0.647\n",
      "[1,   700] loss: 0.609\n",
      "Finished Training\n",
      "Accuracy of plane : 78.18 %\n",
      "Accuracy of   car : 92.00 %\n",
      "Accuracy of  bird : 59.49 %\n",
      "Accuracy of   cat : 52.05 %\n",
      "Accuracy of  deer : 72.73 %\n",
      "Accuracy of   dog : 60.34 %\n",
      "Accuracy of  frog : 78.57 %\n",
      "Accuracy of horse : 84.13 %\n",
      "Accuracy of  ship : 85.96 %\n",
      "Accuracy of truck : 82.05 %\n",
      "Accuracy of the network on the 9984 test images: 75.35 %\n",
      "Time required to run the  file 0:00:22.099833\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer=useOptimizerFunction('SGD')\n",
    "trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.633\n",
      "[1,   200] loss: 0.663\n",
      "[1,   300] loss: 0.611\n",
      "[1,   400] loss: 0.588\n",
      "[1,   500] loss: 0.572\n",
      "[1,   600] loss: 0.560\n",
      "[1,   700] loss: 0.529\n",
      "Finished Training\n",
      "Accuracy of plane : 85.45 %\n",
      "Accuracy of   car : 90.00 %\n",
      "Accuracy of  bird : 62.03 %\n",
      "Accuracy of   cat : 46.58 %\n",
      "Accuracy of  deer : 69.09 %\n",
      "Accuracy of   dog : 65.52 %\n",
      "Accuracy of  frog : 83.93 %\n",
      "Accuracy of horse : 82.54 %\n",
      "Accuracy of  ship : 87.72 %\n",
      "Accuracy of truck : 83.33 %\n",
      "Accuracy of the network on the 9984 test images: 77.34 %\n",
      "Time required to run the  file 0:00:23.223728\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.now()  \n",
    "\n",
    "# NAG optimzer\n",
    "optimizer=useOptimizerFunction('NAG')\n",
    "trainNetwork(max_epoch_num, x_train_tensor,y_train_tensor,optimizer,criterion)\n",
    "AccuracyOfIndividualClassesAndDataset(x_test_tensor,y_test_tensor,batch_size)\n",
    "\n",
    "\n",
    "print('Time required to run the  file', datetime.now() - begin_time)\n",
    "\n",
    "test2 = str ( (datetime.now() - begin_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "89b1052d6e47d8648fd3cb4f0f4d6438dffafe00232db206c8add318d261f991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
